"""
Celery tasks for core app - data retention enforcement, maintenance, and notifications.
"""

import logging
from datetime import timedelta, date

from celery import shared_task
from django.utils import timezone
from django.core.mail import send_mail
from django.template.loader import render_to_string
from django.conf import settings

logger = logging.getLogger(__name__)


@shared_task
def enforce_data_retention():
    """
    Periodic task to enforce data retention policies.

    Checks DataRetentionPolicy records and purges expired data accordingly.
    Should be scheduled via Celery Beat (e.g., daily at 2 AM).
    """
    from .models import DataRetentionPolicy, AuditLog

    policies = DataRetentionPolicy.objects.filter(is_active=True, retention_days__gt=0)
    results = {}

    for policy in policies:
        try:
            cutoff_date = timezone.now() - timedelta(days=policy.retention_days)
            deleted_count = 0

            if policy.data_type == 'audit_logs':
                deleted_count = _purge_audit_logs(cutoff_date)
            elif policy.data_type == 'documents':
                deleted_count = _purge_old_documents(cutoff_date)
            elif policy.data_type == 'analyses':
                deleted_count = _purge_old_analyses(cutoff_date)
            elif policy.data_type == 'session_data':
                deleted_count = _purge_session_data(cutoff_date)

            # Update last cleanup timestamp
            policy.last_cleanup = timezone.now()
            policy.save(update_fields=['last_cleanup'])

            results[policy.data_type] = deleted_count
            logger.info(
                f"Data retention: purged {deleted_count} {policy.data_type} "
                f"older than {policy.retention_days} days"
            )

        except Exception as e:
            logger.error(f"Data retention failed for {policy.data_type}: {e}")
            results[policy.data_type] = f"Error: {e}"

    return results


def _purge_audit_logs(cutoff_date):
    """Purge audit logs older than cutoff date."""
    from .models import AuditLog

    old_logs = AuditLog.objects.filter(timestamp__lt=cutoff_date)
    count = old_logs.count()
    old_logs.delete()
    return count


def _purge_old_documents(cutoff_date):
    """
    Purge soft-deleted documents older than cutoff date.
    Note: This only handles documents that are already soft-deleted.
    """
    from claims.models import Document

    # Only purge documents that were soft-deleted before the cutoff
    old_docs = Document.objects.filter(
        is_deleted=True,
        deleted_at__lt=cutoff_date
    )
    count = old_docs.count()

    for doc in old_docs:
        try:
            # Delete file from storage
            if doc.file:
                doc.file.delete(save=False)
            doc.hard_delete()
        except Exception as e:
            logger.error(f"Failed to purge document {doc.id}: {e}")

    return count


def _purge_old_analyses(cutoff_date):
    """Purge old AI analysis records."""
    try:
        from agents.models import AgentInteraction, DecisionLetterAnalysis, DenialDecoding

        # Delete old denial decodings first (depends on analysis)
        old_decodings = DenialDecoding.objects.filter(
            analysis__created_at__lt=cutoff_date
        )
        decoding_count = old_decodings.count()
        old_decodings.delete()

        # Delete old analyses
        old_analyses = DecisionLetterAnalysis.objects.filter(
            created_at__lt=cutoff_date
        )
        analysis_count = old_analyses.count()
        old_analyses.delete()

        # Delete old agent interactions
        old_interactions = AgentInteraction.objects.filter(
            created_at__lt=cutoff_date
        )
        interaction_count = old_interactions.count()
        old_interactions.delete()

        return decoding_count + analysis_count + interaction_count

    except ImportError:
        logger.warning("agents app not available for analysis purge")
        return 0


def _purge_session_data(cutoff_date):
    """Purge expired session data."""
    from django.contrib.sessions.models import Session

    # Django sessions have an expire_date field
    old_sessions = Session.objects.filter(expire_date__lt=cutoff_date)
    count = old_sessions.count()
    old_sessions.delete()
    return count


@shared_task
def create_default_retention_policies():
    """
    Create default data retention policies if they don't exist.
    Run this once during initial setup.
    """
    from .models import DataRetentionPolicy

    defaults = [
        {
            'data_type': 'audit_logs',
            'retention_days': 365,  # 1 year
            'description': 'Security audit logs retained for compliance',
        },
        {
            'data_type': 'documents',
            'retention_days': 90,  # 90 days after soft delete
            'description': 'Soft-deleted documents purged after 90 days',
        },
        {
            'data_type': 'analyses',
            'retention_days': 180,  # 6 months
            'description': 'AI analysis results retained for 6 months',
        },
        {
            'data_type': 'session_data',
            'retention_days': 30,  # 30 days
            'description': 'Expired sessions cleaned up after 30 days',
        },
    ]

    created = 0
    for policy_data in defaults:
        policy, was_created = DataRetentionPolicy.objects.get_or_create(
            data_type=policy_data['data_type'],
            defaults={
                'retention_days': policy_data['retention_days'],
                'description': policy_data['description'],
                'is_active': True,
            }
        )
        if was_created:
            created += 1
            logger.info(f"Created retention policy: {policy_data['data_type']}")

    return f"Created {created} new retention policies"


# =============================================================================
# EMAIL NOTIFICATION TASKS
# =============================================================================

@shared_task
def send_deadline_reminders():
    """
    Send email reminders for upcoming deadlines.

    Checks all users with upcoming deadlines and sends reminder emails
    based on their notification preferences.

    Should be scheduled via Celery Beat (e.g., daily at 9 AM).
    """
    from .models import Deadline
    from accounts.models import NotificationPreferences

    today = date.today()
    reminders_sent = 0
    errors = []

    # Get all incomplete deadlines that haven't had reminders sent
    upcoming_deadlines = Deadline.objects.filter(
        is_completed=False,
        reminder_sent=False,
        deadline_date__gte=today,
    ).select_related('user', 'claim', 'appeal')

    for deadline in upcoming_deadlines:
        try:
            # Get user's notification preferences
            try:
                prefs = deadline.user.notification_preferences
            except NotificationPreferences.DoesNotExist:
                # Create default preferences if they don't exist
                prefs = NotificationPreferences.objects.create(user=deadline.user)

            # Check if we should send based on days remaining and user preferences
            days_remaining = deadline.days_remaining
            if days_remaining is None:
                continue

            if not prefs.should_send_deadline_reminder(days_remaining):
                continue

            # Send the email
            success = _send_deadline_reminder_email(deadline, days_remaining)

            if success:
                # Mark reminder as sent
                deadline.reminder_sent = True
                deadline.save(update_fields=['reminder_sent', 'updated_at'])

                # Update user's notification tracking
                prefs.last_email_sent = timezone.now()
                prefs.emails_sent_count += 1
                prefs.save(update_fields=['last_email_sent', 'emails_sent_count', 'updated_at'])

                reminders_sent += 1
                logger.info(f"Sent deadline reminder to user_id={deadline.user_id} for deadline_id={deadline.id}")

        except Exception as e:
            error_msg = f"Failed to send reminder for deadline {deadline.id}: {e}"
            logger.error(error_msg)
            errors.append(error_msg)

    result = f"Sent {reminders_sent} deadline reminders"
    if errors:
        result += f" ({len(errors)} errors)"

    logger.info(result)
    return result


@shared_task
def send_exam_reminders():
    """
    Send email reminders for upcoming C&P exams.

    Checks all exam checklists with upcoming exam dates and sends reminder emails
    based on their notification preferences.

    Should be scheduled via Celery Beat (e.g., daily at 9 AM).
    """
    from examprep.models import ExamChecklist
    from accounts.models import NotificationPreferences

    today = date.today()
    reminders_sent = 0
    errors = []

    # Get all incomplete exam checklists with upcoming dates
    upcoming_exams = ExamChecklist.objects.filter(
        exam_completed=False,
        exam_date__gte=today,
    ).select_related('user', 'guidance')

    for exam in upcoming_exams:
        try:
            # Get user's notification preferences
            try:
                prefs = exam.user.notification_preferences
            except NotificationPreferences.DoesNotExist:
                prefs = NotificationPreferences.objects.create(user=exam.user)

            # Check days until exam
            days_until = (exam.exam_date - today).days
            if days_until is None or days_until < 0:
                continue

            if not prefs.should_send_exam_reminder(days_until):
                continue

            # Check if we already sent a reminder for this timing window
            # (We track this in the exam's metadata)
            reminder_key = f"reminder_sent_{days_until}_days"
            metadata = exam.metadata or {}
            if metadata.get(reminder_key):
                continue

            # Send the email
            success = _send_exam_reminder_email(exam, days_until)

            if success:
                # Mark this reminder window as sent
                metadata[reminder_key] = timezone.now().isoformat()
                exam.metadata = metadata
                exam.save(update_fields=['metadata', 'updated_at'])

                # Update user's notification tracking
                prefs.last_email_sent = timezone.now()
                prefs.emails_sent_count += 1
                prefs.save(update_fields=['last_email_sent', 'emails_sent_count', 'updated_at'])

                reminders_sent += 1
                logger.info(f"Sent exam reminder to user_id={exam.user_id} for exam_id={exam.id}")

        except Exception as e:
            error_msg = f"Failed to send reminder for exam {exam.id}: {e}"
            logger.error(error_msg)
            errors.append(error_msg)

    result = f"Sent {reminders_sent} exam reminders"
    if errors:
        result += f" ({len(errors)} errors)"

    logger.info(result)
    return result


def _send_deadline_reminder_email(deadline, days_remaining: int) -> bool:
    """
    Send a deadline reminder email to the user.

    Returns True if email was sent successfully, False otherwise.
    """
    user = deadline.user

    # Determine urgency for email tone
    if days_remaining <= 3:
        urgency = 'critical'
    elif days_remaining <= 7:
        urgency = 'urgent'
    else:
        urgency = 'upcoming'

    # Build email context
    context = {
        'user': user,
        'deadline': deadline,
        'days_remaining': days_remaining,
        'urgency': urgency,
        'site_name': getattr(settings, 'SITE_NAME', 'Benefits Navigator'),
        'site_url': getattr(settings, 'SITE_URL', 'https://benefitsnavigator.com'),
    }

    # Render email templates
    subject = f"{'URGENT: ' if urgency == 'critical' else ''}Deadline Reminder: {deadline.title}"
    text_content = render_to_string('emails/deadline_reminder.txt', context)
    html_content = render_to_string('emails/deadline_reminder.html', context)

    try:
        send_mail(
            subject=subject,
            message=text_content,
            from_email=settings.DEFAULT_FROM_EMAIL,
            recipient_list=[user.email],
            html_message=html_content,
            fail_silently=False,
        )
        return True
    except Exception as e:
        logger.error(f"Failed to send email to user_id={user.id}: {e}")
        return False


def _send_exam_reminder_email(exam, days_until: int) -> bool:
    """
    Send a C&P exam reminder email to the user.

    Returns True if email was sent successfully, False otherwise.
    """
    user = exam.user

    # Determine urgency
    if days_until <= 1:
        urgency = 'tomorrow'
    elif days_until <= 3:
        urgency = 'critical'
    elif days_until <= 7:
        urgency = 'upcoming'
    else:
        urgency = 'scheduled'

    # Build email context
    context = {
        'user': user,
        'exam': exam,
        'days_until': days_until,
        'urgency': urgency,
        'completion_percentage': exam.completion_percentage,
        'site_name': getattr(settings, 'SITE_NAME', 'Benefits Navigator'),
        'site_url': getattr(settings, 'SITE_URL', 'https://benefitsnavigator.com'),
    }

    # Render email templates
    if days_until <= 1:
        subject = f"TOMORROW: Your {exam.condition} C&P Exam"
    elif days_until <= 3:
        subject = f"URGENT: {exam.condition} C&P Exam in {days_until} days"
    else:
        subject = f"Reminder: {exam.condition} C&P Exam in {days_until} days"

    text_content = render_to_string('emails/exam_reminder.txt', context)
    html_content = render_to_string('emails/exam_reminder.html', context)

    try:
        send_mail(
            subject=subject,
            message=text_content,
            from_email=settings.DEFAULT_FROM_EMAIL,
            recipient_list=[user.email],
            html_message=html_content,
            fail_silently=False,
        )
        return True
    except Exception as e:
        logger.error(f"Failed to send email to user_id={user.id}: {e}")
        return False


@shared_task
def send_all_reminders():
    """
    Master task that sends all types of reminders.

    Calls both deadline and exam reminder tasks.
    Should be scheduled via Celery Beat (e.g., daily at 9 AM).
    """
    deadline_result = send_deadline_reminders()
    exam_result = send_exam_reminders()

    return {
        'deadlines': deadline_result,
        'exams': exam_result,
    }


@shared_task
def send_document_analysis_complete_email(document_id: int):
    """
    Send email notification when document analysis is complete.

    Called from claims/tasks.py after process_document_task or
    decode_denial_letter_task completes successfully.
    """
    from claims.models import Document
    from accounts.models import NotificationPreferences

    try:
        document = Document.objects.select_related('user').get(id=document_id)
        user = document.user

        # Get user's notification preferences
        try:
            prefs = user.notification_preferences
        except NotificationPreferences.DoesNotExist:
            prefs = NotificationPreferences.objects.create(user=user)

        # Check if user wants document analysis notifications
        if not prefs.should_send_document_analysis_notification():
            logger.info(f"User user_id={user.id} has disabled document analysis notifications")
            return "Notification disabled by user preferences"

        # Send the email
        success = _send_document_analysis_email(document)

        if success:
            # Update notification tracking
            prefs.last_email_sent = timezone.now()
            prefs.emails_sent_count += 1
            prefs.save(update_fields=['last_email_sent', 'emails_sent_count', 'updated_at'])

            logger.info(f"Sent document analysis complete email to user_id={user.id} for document_id={document_id}")
            return f"Email sent to user_id={user.id}"

        return "Email sending failed"

    except Document.DoesNotExist:
        logger.error(f"Document {document_id} not found")
        return "Document not found"
    except Exception as e:
        logger.error(f"Error sending document analysis email for {document_id}: {e}")
        return f"Error: {e}"


def _send_document_analysis_email(document) -> bool:
    """
    Send a document analysis complete email to the user.

    Returns True if email was sent successfully, False otherwise.
    """
    user = document.user

    # Determine document type label for email (matches Document.DOCUMENT_TYPE_CHOICES)
    doc_type_labels = {
        'medical_records': 'Medical Records',
        'service_records': 'Service Records',
        'decision_letter': 'VA Decision Letter',
        'buddy_statement': 'Buddy Statement',
        'lay_statement': 'Lay Statement',
        'nexus_letter': 'Medical Nexus/Opinion Letter',
        'employment_records': 'Employment Records',
        'personal_statement': 'Personal Statement',
        'other': 'Document',
    }
    doc_type_label = doc_type_labels.get(document.document_type, 'Document')

    # Build email context
    context = {
        'user': user,
        'document': document,
        'doc_type_label': doc_type_label,
        'has_summary': bool(document.ai_summary),
        'page_count': document.page_count or 0,
        'site_name': getattr(settings, 'SITE_NAME', 'Benefits Navigator'),
        'site_url': getattr(settings, 'SITE_URL', 'https://benefitsnavigator.com'),
    }

    # Render email templates
    subject = f"Your {doc_type_label} Analysis is Ready"
    text_content = render_to_string('emails/document_analysis_complete.txt', context)
    html_content = render_to_string('emails/document_analysis_complete.html', context)

    try:
        send_mail(
            subject=subject,
            message=text_content,
            from_email=settings.DEFAULT_FROM_EMAIL,
            recipient_list=[user.email],
            html_message=html_content,
            fail_silently=False,
        )
        return True
    except Exception as e:
        logger.error(f"Failed to send email to user_id={user.id}: {e}")
        return False


# =============================================================================
# HEALTH MONITORING TASKS
# =============================================================================

@shared_task
def record_health_metrics():
    """
    Record system health metrics for historical tracking.

    Should be scheduled via Celery Beat (e.g., every 5 minutes).
    """
    from .health import record_metrics

    try:
        health = record_metrics()
        logger.info(f"Recorded health metrics: status={health['status']}")
        return {
            'status': health['status'],
            'timestamp': health['timestamp'],
        }
    except Exception as e:
        logger.error(f"Failed to record health metrics: {e}")
        return {'error': str(e)}


@shared_task
def check_processing_health():
    """
    Check for processing issues and alert if needed.

    Looks for:
    - High failure rates
    - Documents stuck in processing
    - Celery queue backlog

    Should be scheduled via Celery Beat (e.g., every 15 minutes).
    """
    from .models import ProcessingFailure
    from claims.models import Document

    alerts = []

    # Check failure rate in last hour
    failure_stats = ProcessingFailure.get_failure_stats(hours=1)
    if failure_stats['total'] >= 5:
        alerts.append(f"High failure rate: {failure_stats['total']} failures in last hour")

    # Check for documents stuck in processing
    stuck_threshold = timezone.now() - timezone.timedelta(hours=1)
    stuck_docs = Document.objects.filter(
        status__in=['processing', 'analyzing'],
        updated_at__lt=stuck_threshold
    ).count()

    if stuck_docs > 0:
        alerts.append(f"{stuck_docs} documents stuck in processing for >1 hour")

    # Check Celery queue length
    try:
        from .health import check_celery
        celery_status = check_celery()
        if celery_status.get('queue_length', 0) > 100:
            alerts.append(f"High Celery queue: {celery_status['queue_length']} tasks waiting")
        if celery_status.get('workers', 0) == 0:
            alerts.append("No Celery workers available")
    except Exception as e:
        logger.error(f"Failed to check Celery status: {e}")

    # Send alert if issues found
    if alerts:
        import sentry_sdk
        alert_message = "Processing Health Alert:\n" + "\n".join(f"- {a}" for a in alerts)

        try:
            sentry_sdk.capture_message(
                alert_message,
                level="warning",
                extras={
                    'failure_stats': failure_stats,
                    'stuck_docs': stuck_docs,
                }
            )
        except Exception:
            pass  # Sentry might not be configured

        logger.warning(alert_message)

    return {
        'alerts': alerts,
        'failure_stats': failure_stats,
        'stuck_docs': stuck_docs,
    }


@shared_task
def cleanup_old_health_metrics():
    """
    Clean up old health metrics to prevent database bloat.

    Keeps last 30 days of metrics.
    Should be scheduled via Celery Beat (e.g., daily).
    """
    from .models import SystemHealthMetric

    threshold = timezone.now() - timezone.timedelta(days=30)
    deleted, _ = SystemHealthMetric.objects.filter(
        timestamp__lt=threshold
    ).delete()

    logger.info(f"Cleaned up {deleted} old health metrics")
    return f"Deleted {deleted} old metrics"


@shared_task
def run_monitoring_checks():
    """
    Run all monitoring checks and send alerts if thresholds exceeded.

    Checks:
    - System health (database, Redis, Celery, document processing)
    - Download anomalies (potential data exfiltration)
    - Task age (stale/long-running tasks)

    Should be scheduled via Celery Beat (e.g., every 5 minutes).
    """
    from core.alerting import run_all_monitoring_checks

    results = run_all_monitoring_checks()

    total_alerts = (
        len(results.get('health_alerts', [])) +
        len(results.get('download_anomalies', [])) +
        len(results.get('stale_tasks', []))
    )

    return {
        'total_alerts': total_alerts,
        'results': results,
    }


@shared_task
def check_download_anomalies_task(hours: int = 1):
    """
    Check for unusual download patterns.

    Detects potential data exfiltration by monitoring:
    - High download volume per user
    - Multiple users from same IP
    - Unusual download patterns

    Should be scheduled via Celery Beat (e.g., hourly).
    """
    from core.alerting import check_download_anomalies

    anomalies = check_download_anomalies(hours=hours)

    if anomalies:
        logger.warning(f"Detected {len(anomalies)} download anomalies")

    return {
        'anomalies_detected': len(anomalies),
        'details': anomalies,
    }
